{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/06 17:05:43 WARN Utils: Your hostname, MacBook-Pro-di-Francesco-4.local resolves to a loopback address: 127.0.0.1; using 192.168.1.181 instead (on interface en0)\n",
      "23/12/06 17:05:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/06 17:05:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFile(filename):\n",
    "    # Initialize Spark context\n",
    "    #spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "\n",
    "    #sc = spark.sparkContext\n",
    "\n",
    "    # Read the file into an RDD\n",
    "    # Each line in the file becomes one record in the RDD\n",
    "    lines = sc.textFile(filename)\n",
    "\n",
    "    # Process each line in the RDD\n",
    "    data = lines.map(lambda line: line.split(','))\\\n",
    "                .map(lambda elem: ([float(x) for x in elem[:-1]], int(elem[-1])))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9.012784269851089, 1672.9999766891833, 21.99998846107087, 0.9999997452701503, 61.99988768910407, 69.99980788311223, 13.000000232993186, 2.9999999785934133, 199.00000192395984, 2468369573.0148935, 2468372549.224571], 1)\n",
      "([3599.9990884099398, 48206.57583515873, 13362.999847123127, 1.0000019205086303, 262.99924185046257, 82.99988759205826, 13.999945225127506, 5.000000032871429, 216.99999612378573, 1539044199.5873349, 2468368394.7593513], 0)\n",
      "([0.0006999386757797765, 0.0009472844685660675, 53.000260213743786, 2.0000004196928174, 646242903.4069201, 82.99988759205826, 12.999999992936175, 2.999999985175615, 216.99999612378573, 2468369544.907854, 2503250078.1888514], 0)\n",
      "([3599.9990884099398, 1718.7743566535646, 63377.68872854763, 1.0000019205086303, 926682.197305462, 543.5568914038613, 12.999999992936175, 5.000000032871429, 186.99999400676145, 405749204.8178947, 1125424493.3033822], 0)\n",
      "([3471.3792185025277, 48024.1859162382, 0.00035088937602267833, 507008.9762772573, 262.99924185046257, 1000.1236755265327, 12.999999992936175, -7.710424743123667e-09, 7.999999237579246, 461400456.66477466, 98.30836248397827], 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Testing of the read function\n",
    "filename = 'data/botnet_tot_syn_l.csv'\n",
    "RDD_Xy = readFile(filename)\n",
    "\n",
    "# Printing first 5 rows of the RDD\n",
    "for row in RDD_Xy.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_stats(RDD):\\n    # Calculate sum and sum of squares for each feature\\n    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\\n    return sums\\nprint(compute_stats(rdd))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compute_stats(RDD):\n",
    "    # Calculate sum and sum of squares for each feature\n",
    "    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\n",
    "    return sums\n",
    "print(compute_stats(rdd))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_stds(RDD):\n",
    "    # Calculate sum and sum of squares for each feature\n",
    "    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))]) \n",
    "    sum_of_squares = RDD.map(lambda x: [xi ** 2 for xi in x[0]]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\n",
    "    count = RDD.count()\n",
    "\n",
    "    # Compute mean and standard deviation for each feature\n",
    "    means = [sums[i] / count for i in range(len(sums))]\n",
    "    stds = [(sum_of_squares[i] / count - (means[i]) ** 2) ** 0.5 for i in range(len(means))]\n",
    "    \n",
    "    return means, stds\n",
    "\n",
    "\n",
    "\n",
    "def normalize(RDD_Xy):\n",
    "    # Compute means and standard deviations\n",
    "    means, stds = compute_mean_stds(RDD_Xy)\n",
    "\n",
    "    # Broadcast the means and standard deviations\n",
    "    bc_means = sc.broadcast(means)\n",
    "    bc_stds = sc.broadcast(stds)\n",
    "\n",
    "    # Normalize each feature\n",
    "    def normalize_features(features, means, stds):\n",
    "        return [(features[i] - means[i]) / stds[i] if stds[i] != 0 else 0 for i in range(len(features))]\n",
    "        #return [(features[i] - means[i]) / stds[i] for i in range(len(features))]\n",
    "\n",
    "    normalized_RDD = RDD_Xy.map(lambda x: (normalize_features(x[0], bc_means.value, bc_stds.value), x[1]))\n",
    "\n",
    "    return normalized_RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-0.792409774778866, -0.8130937083307339, -0.4224507642218906, -0.4664697491193844, -0.5223929554497563, -0.35631956924945285, 0.7370102960327225, 0.5283496333423401, 0.8271779941619601, 0.4731661571170488, 0.15895172357621995], 1)\n",
      "([1.443720404002351, 1.1163610034798641, 0.39127563577819524, -0.4664697491031832, -0.5223920961870128, -0.35631927462193325, 0.9263319360863808, 1.480120748267978, 1.0264183320400917, -0.8286954523619166, 0.15894853159524694], 0)\n",
      "([-0.7980216574316705, -0.8824624555329105, -0.4205599209717913, -0.46646230111358483, 2.240264662197529, -0.35631927462193325, 0.7370102505822355, 0.5283496364747148, 1.0264183320400917, 0.4731661177428138, 0.185749013466657], 0)\n",
      "([1.443720404002351, -0.8111957326975887, 3.441892018491165, -0.4664697491031832, -0.5184316978805608, -0.35630883678072567, 0.7370102505822355, 1.480120748267978, 0.6943509718073647, -2.416291452741655, -0.8728693208325027], 0)\n",
      "([1.3636279869526706, 1.108798440707592, -0.42379261911074273, 3.3097260518708533, -0.5223920961870128, -0.3562984893719275, 0.7370102505822355, -0.8993069937831265, -1.2869840798639645, -2.3383314288679005, -1.7375614208186567], 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Testing of the normalize function\n",
    "\n",
    "normalized_RDD = normalize(RDD_Xy).cache()\n",
    "\n",
    "# Print first 5 normalized rows\n",
    "for row in normalized_RDD.take(5):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def linear_combination(features, weights, bias):\n",
    "    return sum(weights[j] * features[j] for j in range(len(weights))) + bias\n",
    "\n",
    "def compute_gradients_and_cost(record, weights, bias):\n",
    "    xi, yi = record\n",
    "    y_hat = sigmoid(linear_combination(xi, weights, bias))\n",
    "    error = y_hat - yi\n",
    "    dw = np.zeros(len(weights))\n",
    "    for j in range(len(weights)):\n",
    "        dw[j] = error * xi[j]\n",
    "    db = error\n",
    "    cost_contribution = -yi * np.log(y_hat) - (1 - yi) * np.log(1 - y_hat)\n",
    "    return (dw, db, cost_contribution)\n",
    "\n",
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    np.random.seed(0) \n",
    "    feature_count = len(RDD_Xy.first()[0])\n",
    "    w = np.random.rand(feature_count)\n",
    "    b = np.random.rand()\n",
    "    m = RDD_Xy.count()\n",
    "\n",
    "    for n in range(iterations):\n",
    "        bc_w = sc.broadcast(w)\n",
    "        bc_b = sc.broadcast(b)\n",
    "\n",
    "        gradients_and_cost = RDD_Xy.map(lambda x: compute_gradients_and_cost(x, bc_w.value, bc_b.value))\n",
    "        dw, db, total_cost = gradients_and_cost.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * ((dw / m) + lambda_reg * w)\n",
    "        b -= learning_rate * db / m\n",
    "\n",
    "        reg_cost = (lambda_reg / (2 * feature_count)) * np.sum(np.square(w))\n",
    "        cost = (total_cost / m) + reg_cost\n",
    "        print(f\"Iteration {n+1}/{iterations} - Cost: {cost}\")\n",
    "\n",
    "        bc_w.unpersist()\n",
    "        bc_b.unpersist()\n",
    "\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.4998030671352984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7452491273041107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4463838653243955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.33598210709531867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.2858186861976214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.2581741737711223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.24076133573757508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22876056610476408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.21995387544017125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21318893422096608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now use this smaller RDD for training\n",
    "w, b = train(normalized_RDD, 10, 1.5, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19840144016729716\n",
      "0.016435331016430403\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "print(w[0])\n",
    "print(b)\n",
    "print(w.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, x):\n",
    "    z = 0\n",
    "    for j in range(len(w)):\n",
    "        z += w[j] * x[j]\n",
    "\n",
    "    z+= b\n",
    "    # Compute the sigmoid of z\n",
    "    y_hat = sigmoid(z)\n",
    "\n",
    "    if y_hat > 0.5:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, b, RDD_Xy):\n",
    "    \n",
    "    results = RDD_Xy.map(lambda r: 1 if predict(w, b, r[0]) == r[1] else 0)\n",
    "    results = results.reduce( lambda a, b: a + b)\n",
    "\n",
    "    accuracy = results / RDD_Xy.count()\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.930181\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(w, b, normalized_RDD)\n",
    "print(\"Accuracy: \",acc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional function to assign fold indices\n",
    "def assign_fold_index(RDD_Xy, num_folds):\n",
    "    num_samples = RDD_Xy.count()\n",
    "    fold_sizes = (num_samples // num_folds) * np.ones(num_folds, dtype=int)\n",
    "    fold_sizes[:num_samples % num_folds] += 1\n",
    "    fold_indices = np.hstack([np.full(fold_size, fold_index) for fold_index, fold_size in enumerate(fold_sizes)])\n",
    "    np.random.shuffle(fold_indices)\n",
    "    return RDD_Xy.zipWithIndex().map(lambda x: (fold_indices[x[1]], x[0]))\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(RDD_Xy, num_folds, iterations, learning_rate, lambda_reg):\n",
    "    # Assign each row to a fold\n",
    "    indexed_RDD = assign_fold_index(RDD_Xy, num_folds)\n",
    "\n",
    "    # Broadcast the indexed RDD\n",
    "    bc_indexed_RDD = sc.broadcast(indexed_RDD.collect())\n",
    "\n",
    "    # Initialize array to hold accuracy for each fold\n",
    "    fold_accuracies = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for fold in range(num_folds):\n",
    "        train_RDD = sc.parallelize([x[1] for x in bc_indexed_RDD.value if x[0] != fold])\n",
    "        test_RDD = sc.parallelize([x[1] for x in bc_indexed_RDD.value if x[0] == fold])\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        w, b = train(train_RDD, iterations, learning_rate, lambda_reg)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        fold_acc = accuracy(w, b, test_RDD)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Fold {fold+1}/{num_folds} - Accuracy: {fold_acc}\")\n",
    "\n",
    "    # Calculate average accuracy across all folds\n",
    "    avg_acc = sum(fold_accuracies) / num_folds\n",
    "    return avg_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.499794021656618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7451735280730557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.44635985587310784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.3359381192787715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.28577209507387863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.25812793967356235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.2407149549913496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22871340251907202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.219905621869102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21313952713358483\n",
      "Fold 1/10 - Accuracy: 0.92975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.5005540928897256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7454112650746967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4462041522017837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.335785039608588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.28566580795421764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.25806346903956484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.2406835150235894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22870783731943445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.2199205525540362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21317094141315468\n",
      "Fold 2/10 - Accuracy: 0.92985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.4993483106806924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7454300742413026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4465744606087578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.3361716845770645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.28600946873379773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.25836536102276897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.2409537820712884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.2289550947416852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.22015097961959548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21338884254985194\n",
      "Fold 3/10 - Accuracy: 0.93191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.5000744061980968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7452399361295041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4464097157826183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.33606832178704493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.28594416345721946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.25832402212497285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.2409265177835003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22893582329332882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.22013604886118177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.2133760462188594\n",
      "Fold 4/10 - Accuracy: 0.93121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.4995483390713462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7450526030781884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4464389704166971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.33612690323213007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.28597739963676116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.2583328744875979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.2409189272612019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.2289178534961806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.22011161430550458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21334759595771857\n",
      "Fold 5/10 - Accuracy: 0.93039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.5003983171240927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7453537540094229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.44630081792642895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.3358617633478102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.2856709276089047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.25800274056708633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.24057172131603677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22855746375704203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.21974068553895312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/10 - Cost: 0.21296801183412686\n",
      "Fold 6/10 - Accuracy: 0.92856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.4999729194723457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/06 17:07:35 ERROR Inbox: An error happened while processing message in the inbox for LocalSchedulerBackendEndpoint\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1896)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1805)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1202)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:354)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$1417/0x0000000801349a48.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1408/0x0000000801343a48.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1406/0x0000000801342f98.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1405/0x0000000801342bb8.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1404/0x00000008013427d8.apply(Unknown Source)\n",
      "Exception in thread \"dispatcher-event-loop-0\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1896)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1805)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1202)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:354)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:530)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:494)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$1417/0x0000000801349a48.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:470)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:414)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1408/0x0000000801343a48.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:409)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1406/0x0000000801342f98.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:399)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:606)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1405/0x0000000801342bb8.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:601)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1404/0x00000008013427d8.apply(Unknown Source)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 7) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Call the cross-validation function\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m avg_accuracy \u001b[39m=\u001b[39m cross_validate(normalized_RDD, \u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m1.5\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage Accuracy: \u001b[39m\u001b[39m\"\u001b[39m, avg_accuracy)\n",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m test_RDD \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mparallelize([x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m bc_indexed_RDD\u001b[39m.\u001b[39mvalue \u001b[39mif\u001b[39;00m x[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m fold])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Train the model on the training set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m w, b \u001b[39m=\u001b[39m train(train_RDD, iterations, learning_rate, lambda_reg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m fold_acc \u001b[39m=\u001b[39m accuracy(w, b, test_RDD)\n",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m bc_b \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mbroadcast(b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m gradients_and_cost \u001b[39m=\u001b[39m RDD_Xy\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: compute_gradients_and_cost(x, bc_w\u001b[39m.\u001b[39mvalue, bc_b\u001b[39m.\u001b[39mvalue))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m dw, db, total_cost \u001b[39m=\u001b[39m gradients_and_cost\u001b[39m.\u001b[39;49mreduce(\u001b[39mlambda\u001b[39;49;00m a, b: (a[\u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m b[\u001b[39m0\u001b[39;49m], a[\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m b[\u001b[39m1\u001b[39;49m], a[\u001b[39m2\u001b[39;49m] \u001b[39m+\u001b[39;49m b[\u001b[39m2\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Update weights and bias\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X54sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m w \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m ((dw \u001b[39m/\u001b[39m m) \u001b[39m+\u001b[39m lambda_reg \u001b[39m*\u001b[39m w)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[39myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   1925\u001b[0m \u001b[39mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[39mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                                                        (0 + 7) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Call the cross-validation function\n",
    "avg_accuracy = cross_validate(normalized_RDD, 10, 10, 1.5, 0)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/06 16:57:00 WARN BlockManager: Task 201 already completed, not releasing lock for rdd_29_0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 5.0 in stage 26.0 (TID 207)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 7.0 in stage 26.0 (TID 209)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 2.0 in stage 26.0 (TID 204)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 202)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 3.0 in stage 26.0 (TID 205)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 6.0 in stage 26.0 (TID 208)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 1.0 in stage 26.0 (TID 203)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/Users/francescomattioli/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/12/06 16:57:01 ERROR Executor: Exception in task 4.0 in stage 26.0 (TID 206)\n",
      "org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "23/12/06 16:57:01 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 202) (192.168.1.181 executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "23/12/06 16:57:01 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n",
      "23/12/06 16:57:01 WARN TaskSetManager: Lost task 8.0 in stage 26.0 (TID 210) (192.168.1.181 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 202) (192.168.1.181 executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 202) (192.168.1.181 executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m RDD_Xy \u001b[39m=\u001b[39m readFile(\u001b[39m'\u001b[39m\u001b[39mdata/botnet_tot_syn_l.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m normalized_RDD \u001b[39m=\u001b[39m normalize(RDD_Xy)\u001b[39m.\u001b[39mcache()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m best_hyperparameters \u001b[39m=\u001b[39m cross_validation_hyperparameter_tuning(normalized_RDD, learning_rates, lambda_values, iteration_counts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLearning Rate:\u001b[39m\u001b[39m\"\u001b[39m, best_hyperparameters[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_blocks_cv):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     tr_data, test_data \u001b[39m=\u001b[39m get_block_data(RDD_with_indices, i)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     params \u001b[39m=\u001b[39m train(tr_data, iterations, learning_rate, lambda_reg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     w, b \u001b[39m=\u001b[39m params[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], params[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     acc \u001b[39m=\u001b[39m accuracy(w, b, test_data)\n",
      "\u001b[1;32m/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO ANNO/PRIMO SEMESTRE/MASSIVELY PARALLEL MACHINE LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic Regression/CV_spark_version.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(feature_count)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m m \u001b[39m=\u001b[39m RDD_Xy\u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescomattioli/Library/CloudStorage/OneDrive-PolitecnicodiMilano/UNI/MAGISTRALE/SECONDO%20ANNO/PRIMO%20SEMESTRE/MASSIVELY%20PARALLEL%20MACHINE%20LEARNING/Project/Massively_Parallel_Machine_Learning/Logistic%20Regression/CV_spark_version.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     bc_w \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mbroadcast(w)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   2293\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   2045\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MPML/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 202) (192.168.1.181 executor driver): org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition\n\tat org.apache.spark.errors.SparkCoreErrors$.canOnlyZipRDDsWithSamePartitionSizeError(SparkCoreErrors.scala:142)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:971)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:967)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def assign_random_indices(RDD):\n",
    "    total_count = RDD.count()\n",
    "    random_indices = RDD.context.parallelize(range(total_count), RDD.getNumPartitions())\\\n",
    "                                .map(lambda x: random.randint(0, 9))\n",
    "    return RDD.zip(random_indices)\n",
    "\n",
    "def get_block_data(RDD, block_index):\n",
    "    # Use flatMap to create training and test sets\n",
    "    def split_data(item):\n",
    "        (record, index) = item\n",
    "        if index == block_index:\n",
    "            return [(record, 'test')]\n",
    "        else:\n",
    "            return [(record, 'train')]\n",
    "\n",
    "    split_RDD = RDD.flatMap(split_data)\n",
    "    # Separate training and test data\n",
    "    tr_data = split_RDD.flatMap(lambda x: [x[0]] if x[1] == 'train' else [])\n",
    "    test_data = split_RDD.flatMap(lambda x: [x[0]] if x[1] == 'test' else [])\n",
    "    return tr_data, test_data\n",
    "\n",
    "\n",
    "def cross_validation_hyperparameter_tuning(RDD, learning_rates, lambda_values, iteration_counts):\n",
    "    best_params = {'learning_rate': 0, 'lambda_reg': 0, 'iterations': 0, 'accuracy': 0}\n",
    "    num_blocks_cv = 10\n",
    "\n",
    "    RDD_with_indices = assign_random_indices(RDD)\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        for lambda_reg in lambda_values:\n",
    "            for iterations in iteration_counts:\n",
    "                accuracies = []\n",
    "\n",
    "                for i in range(num_blocks_cv):\n",
    "                    tr_data, test_data = get_block_data(RDD_with_indices, i)\n",
    "                    params = train(tr_data, iterations, learning_rate, lambda_reg)\n",
    "                    w, b = params[:-1], params[-1]\n",
    "                    acc = accuracy(w, b, test_data)\n",
    "                    accuracies.append(acc)\n",
    "\n",
    "                avg_acc = sum(accuracies) / num_blocks_cv\n",
    "                if avg_acc > best_params['accuracy']:\n",
    "                    best_params['accuracy'] = avg_acc\n",
    "                    best_params['learning_rate'] = learning_rate\n",
    "                    best_params['lambda_reg'] = lambda_reg\n",
    "                    best_params['iterations'] = iterations\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Hyperparameter ranges\n",
    "learning_rates = [0.1, 1, 1.5]\n",
    "lambda_values = [0.01, 0.1, 1]\n",
    "iteration_counts = [10]\n",
    "\n",
    "# Perform cross-validation with hyperparameter tuning\n",
    "RDD_Xy = readFile('data/botnet_tot_syn_l.csv')\n",
    "normalized_RDD = normalize(RDD_Xy).cache()\n",
    "best_hyperparameters = cross_validation_hyperparameter_tuning(normalized_RDD, learning_rates, lambda_values, iteration_counts)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Learning Rate:\", best_hyperparameters['learning_rate'])\n",
    "print(\"Lambda:\", best_hyperparameters['lambda_reg'])\n",
    "print(\"Iterations:\", best_hyperparameters['iterations'])\n",
    "print(\"Accuracy:\", best_hyperparameters['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
