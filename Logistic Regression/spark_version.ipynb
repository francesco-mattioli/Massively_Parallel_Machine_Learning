{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/08 09:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFile(filename):\n",
    "    # Initialize Spark context\n",
    "    #spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "\n",
    "    #sc = spark.sparkContext\n",
    "\n",
    "    # Read the file into an RDD\n",
    "    # Each line in the file becomes one record in the RDD\n",
    "    lines = sc.textFile(filename)\n",
    "\n",
    "    # Process each line in the RDD\n",
    "    data = lines.map(lambda line: line.split(','))\\\n",
    "                .map(lambda elem: ([float(x) for x in elem[:-1]], int(elem[-1])))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9.012784269851089, 1672.9999766891833, 21.99998846107087, 0.9999997452701503, 61.99988768910407, 69.99980788311223, 13.000000232993186, 2.9999999785934133, 199.00000192395984, 2468369573.0148935, 2468372549.224571], 1)\n",
      "([3599.9990884099398, 48206.57583515873, 13362.999847123127, 1.0000019205086303, 262.99924185046257, 82.99988759205826, 13.999945225127506, 5.000000032871429, 216.99999612378573, 1539044199.5873349, 2468368394.7593513], 0)\n",
      "([0.0006999386757797765, 0.0009472844685660675, 53.000260213743786, 2.0000004196928174, 646242903.4069201, 82.99988759205826, 12.999999992936175, 2.999999985175615, 216.99999612378573, 2468369544.907854, 2503250078.1888514], 0)\n",
      "([3599.9990884099398, 1718.7743566535646, 63377.68872854763, 1.0000019205086303, 926682.197305462, 543.5568914038613, 12.999999992936175, 5.000000032871429, 186.99999400676145, 405749204.8178947, 1125424493.3033822], 0)\n",
      "([3471.3792185025277, 48024.1859162382, 0.00035088937602267833, 507008.9762772573, 262.99924185046257, 1000.1236755265327, 12.999999992936175, -7.710424743123667e-09, 7.999999237579246, 461400456.66477466, 98.30836248397827], 0)\n"
     ]
    }
   ],
   "source": [
    "#Testing of the read function\n",
    "filename = 'data/botnet_tot_syn_l.csv'\n",
    "RDD_Xy = readFile(filename)\n",
    "\n",
    "# Printing first 5 rows of the RDD\n",
    "for row in RDD_Xy.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_stats(RDD):\\n    # Calculate sum and sum of squares for each feature\\n    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\\n    return sums\\nprint(compute_stats(rdd))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compute_stats(RDD):\n",
    "    # Calculate sum and sum of squares for each feature\n",
    "    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\n",
    "    return sums\n",
    "print(compute_stats(rdd))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_stds(RDD):\n",
    "    # Calculate sum and sum of squares for each feature\n",
    "    sums = RDD.map(lambda x: x[0]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))]) \n",
    "    sum_of_squares = RDD.map(lambda x: [xi ** 2 for xi in x[0]]).reduce(lambda a, b: [a[i] + b[i] for i in range(len(a))])\n",
    "    count = RDD.count()\n",
    "\n",
    "    # Compute mean and standard deviation for each feature\n",
    "    means = [sums[i] / count for i in range(len(sums))]\n",
    "    stds = [(sum_of_squares[i] / count - (means[i]) ** 2) ** 0.5 for i in range(len(means))]\n",
    "    \n",
    "    return means, stds\n",
    "\n",
    "\n",
    "\n",
    "def normalize(RDD_Xy):\n",
    "    # Compute means and standard deviations\n",
    "    means, stds = compute_mean_stds(RDD_Xy)\n",
    "\n",
    "    # Broadcast the means and standard deviations\n",
    "    bc_means = sc.broadcast(means)\n",
    "    bc_stds = sc.broadcast(stds)\n",
    "\n",
    "    # Normalize each feature\n",
    "    def normalize_features(features, means, stds):\n",
    "        return [(features[i] - means[i]) / stds[i] if stds[i] != 0 else 0 for i in range(len(features))]\n",
    "        #return [(features[i] - means[i]) / stds[i] for i in range(len(features))]\n",
    "\n",
    "    normalized_RDD = RDD_Xy.map(lambda x: (normalize_features(x[0], bc_means.value, bc_stds.value), x[1]))\n",
    "\n",
    "    bc_means.unpersist()\n",
    "    bc_stds.unpersist()\n",
    "\n",
    "\n",
    "    return normalized_RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-0.792409774778866, -0.8130937083307339, -0.4224507642218906, -0.4664697491193844, -0.5223929554497563, -0.35631956924945285, 0.7370102960327225, 0.5283496333423401, 0.8271779941619601, 0.4731661571170488, 0.15895172357621995], 1)\n",
      "([1.443720404002351, 1.1163610034798641, 0.39127563577819524, -0.4664697491031832, -0.5223920961870128, -0.35631927462193325, 0.9263319360863808, 1.480120748267978, 1.0264183320400917, -0.8286954523619166, 0.15894853159524694], 0)\n",
      "([-0.7980216574316705, -0.8824624555329105, -0.4205599209717913, -0.46646230111358483, 2.240264662197529, -0.35631927462193325, 0.7370102505822355, 0.5283496364747148, 1.0264183320400917, 0.4731661177428138, 0.185749013466657], 0)\n",
      "([1.443720404002351, -0.8111957326975887, 3.441892018491165, -0.4664697491031832, -0.5184316978805608, -0.35630883678072567, 0.7370102505822355, 1.480120748267978, 0.6943509718073647, -2.416291452741655, -0.8728693208325027], 0)\n",
      "([1.3636279869526706, 1.108798440707592, -0.42379261911074273, 3.3097260518708533, -0.5223920961870128, -0.3562984893719275, 0.7370102505822355, -0.8993069937831265, -1.2869840798639645, -2.3383314288679005, -1.7375614208186567], 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Testing of the normalize function\n",
    "\n",
    "normalized_RDD = normalize(RDD_Xy).cache()\n",
    "\n",
    "# Print first 5 normalized rows\n",
    "for row in normalized_RDD.take(5):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef sigmoid(z):\\n    return 1 / (1 + np.exp(-z))\\n\\ndef linear_combination(features, weights, bias):\\n    return sum(weights[j] * features[j] for j in range(len(weights))) + bias\\n\\ndef compute_cost(RDD, w, b, lambda_reg, m):\\n    \\n    bc_w = sc.broadcast(w)\\n    bc_b = sc.broadcast(b)\\n\\n    total_cost = RDD.map(lambda x: (x[1] * np.log(sigmoid(linear_combination(x[0], bc_w.value, bc_b.value))) + (1 - x[1]) * np.log(1 - sigmoid(linear_combination(x[0], bc_w.value, bc_b.value)))\\n    )).reduce(lambda a, b: a + b)\\n    reg_cost = (lambda_reg / (2 * 11)) * np.sum(np.square(w))\\n    cost = (- total_cost / m) + reg_cost\\n\\n    #bc_w.unpersist()\\n    #bc_b.unpersist()\\n    \\n    return cost\\n\\ndef compute_gradients(record, weights, bias):\\n    xi, yi = record\\n    y_hat = sigmoid(linear_combination(xi, weights, bias))\\n    error = y_hat - yi\\n    dw = np.array([error * xi[j] for j in range(len(weights))])\\n    db = error\\n    return dw, db\\n\\ndef train(RDD_Xy, iterations, learning_rate, lambda_reg):\\n    np.random.seed(42) \\n    feature_count = len(RDD_Xy.first()[0])\\n    w = np.random.rand(feature_count)\\n    b = np.random.rand()\\n    m = RDD_Xy.count()\\n\\n    for n in range(iterations):\\n        bc_w = sc.broadcast(w)\\n        bc_b = sc.broadcast(b)\\n\\n        dw, db = RDD_Xy.map(lambda x: compute_gradients(x, bc_w.value, bc_b.value)).reduce(lambda a, b: (np.add(a[0], b[0]), a[1] + b[1]))\\n        \\n\\n        # Update weights and bias\\n        w -= learning_rate * ((dw / m) + lambda_reg * w)\\n        b -= learning_rate * db / m\\n\\n        # Compute and print cost\\n        cost = compute_cost(RDD_Xy, w, b, lambda_reg, m)\\n        print(f\"Iteration {n+1}/{iterations} - Cost: {cost}\")\\n\\n        #bc_w.unpersist()\\n        #bc_b.unpersist()\\n\\n    return w, b\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def linear_combination(features, weights, bias):\n",
    "    return sum(weights[j] * features[j] for j in range(len(weights))) + bias\n",
    "\n",
    "def compute_cost(RDD, w, b, lambda_reg, m):\n",
    "    \n",
    "    bc_w = sc.broadcast(w)\n",
    "    bc_b = sc.broadcast(b)\n",
    "\n",
    "    total_cost = RDD.map(lambda x: (x[1] * np.log(sigmoid(linear_combination(x[0], bc_w.value, bc_b.value))) + (1 - x[1]) * np.log(1 - sigmoid(linear_combination(x[0], bc_w.value, bc_b.value)))\n",
    "    )).reduce(lambda a, b: a + b)\n",
    "    reg_cost = (lambda_reg / (2 * 11)) * np.sum(np.square(w))\n",
    "    cost = (- total_cost / m) + reg_cost\n",
    "\n",
    "    #bc_w.unpersist()\n",
    "    #bc_b.unpersist()\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def compute_gradients(record, weights, bias):\n",
    "    xi, yi = record\n",
    "    y_hat = sigmoid(linear_combination(xi, weights, bias))\n",
    "    error = y_hat - yi\n",
    "    dw = np.array([error * xi[j] for j in range(len(weights))])\n",
    "    db = error\n",
    "    return dw, db\n",
    "\n",
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    np.random.seed(42) \n",
    "    feature_count = len(RDD_Xy.first()[0])\n",
    "    w = np.random.rand(feature_count)\n",
    "    b = np.random.rand()\n",
    "    m = RDD_Xy.count()\n",
    "\n",
    "    for n in range(iterations):\n",
    "        bc_w = sc.broadcast(w)\n",
    "        bc_b = sc.broadcast(b)\n",
    "\n",
    "        dw, db = RDD_Xy.map(lambda x: compute_gradients(x, bc_w.value, bc_b.value)).reduce(lambda a, b: (np.add(a[0], b[0]), a[1] + b[1]))\n",
    "        \n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * ((dw / m) + lambda_reg * w)\n",
    "        b -= learning_rate * db / m\n",
    "\n",
    "        # Compute and print cost\n",
    "        cost = compute_cost(RDD_Xy, w, b, lambda_reg, m)\n",
    "        print(f\"Iteration {n+1}/{iterations} - Cost: {cost}\")\n",
    "\n",
    "        #bc_w.unpersist()\n",
    "        #bc_b.unpersist()\n",
    "\n",
    "    return w, b\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def linear_combination(features, weights, bias):\n",
    "    return sum(weights[j] * features[j] for j in range(len(weights))) + bias\n",
    "\n",
    "def compute_gradients_and_cost(record, weights, bias):\n",
    "    xi, yi = record\n",
    "    y_hat = sigmoid(linear_combination(xi, weights, bias))\n",
    "    error = y_hat - yi\n",
    "    dw = np.zeros(len(weights))\n",
    "    for j in range(len(weights)):\n",
    "        dw[j] = error * xi[j]\n",
    "    db = error\n",
    "    cost_contribution = -yi * np.log(y_hat) - (1 - yi) * np.log(1 - y_hat)\n",
    "    return (dw, db, cost_contribution)\n",
    "\n",
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    np.random.seed(0) \n",
    "    feature_count = len(RDD_Xy.first()[0])\n",
    "    w = np.random.rand(feature_count)\n",
    "    b = np.random.rand()\n",
    "    m = RDD_Xy.count()\n",
    "\n",
    "    for n in range(iterations):\n",
    "        bc_w = sc.broadcast(w)\n",
    "        bc_b = sc.broadcast(b)\n",
    "\n",
    "        gradients_and_cost = RDD_Xy.map(lambda x: compute_gradients_and_cost(x, bc_w.value, bc_b.value))\n",
    "        dw, db, total_cost = gradients_and_cost.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "        \n",
    "        reg_cost = (lambda_reg / (2 * feature_count)) * np.sum(np.square(w))\n",
    "        cost = (total_cost / m) + reg_cost\n",
    "        \n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * ((dw / m) + (lambda_reg/m) * w)\n",
    "        b -= learning_rate * db / m\n",
    "\n",
    "\n",
    "        print(f\"Iteration {n+1}/{iterations} - Cost: {cost}\")\n",
    "\n",
    "        bc_w.unpersist()\n",
    "        bc_b.unpersist()\n",
    "\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10 - Cost: 1.4998030671352984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/10 - Cost: 0.7452491273041107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/10 - Cost: 0.4463838653243955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4/10 - Cost: 0.33598210709531867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/10 - Cost: 0.2858186861976214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/10 - Cost: 0.2581741737711223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7/10 - Cost: 0.24076133573757508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8/10 - Cost: 0.22876056610476408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9/10 - Cost: 0.21995387544017125\n",
      "Iteration 10/10 - Cost: 0.21318893422096608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now use this smaller RDD for training\n",
    "w, b = train(normalized_RDD, 10, 1.5, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19840144016729716\n",
      "0.016435331016430403\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "print(w[0])\n",
    "print(b)\n",
    "print(w.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, x):\n",
    "    z = 0\n",
    "    for j in range(len(w)):\n",
    "        z += w[j] * x[j]\n",
    "\n",
    "    z+= b\n",
    "    # Compute the sigmoid of z\n",
    "    y_hat = sigmoid(z)\n",
    "\n",
    "    if y_hat > 0.5:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, b, RDD_Xy):\n",
    "    \n",
    "    results = RDD_Xy.map(lambda r: 1 if predict(w, b, r[0]) == r[1] else 0)\n",
    "    results = results.reduce( lambda a, b: a + b)\n",
    "\n",
    "    accuracy = results / RDD_Xy.count()\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.930181\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(w, b, normalized_RDD)\n",
    "print(\"Accuracy: \",acc )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
